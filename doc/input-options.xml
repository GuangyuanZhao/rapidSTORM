<?xml version="1.0" encoding="iso-8859-1"?>
<!DOCTYPE chapter
[<!ENTITY % isogrk1 PUBLIC "ISO 8879:1986//ENTITIES Greek Letters//EN//XML"
                    "http://www.oasis-open.org/docbook/xmlcharent/0.3/iso-grk1.ent">
 <!ENTITY % isopub PUBLIC "ISO 8879:1986//ENTITIES Publishing//EN//XML"
                    "http://www.oasis-open.org/docbook/xmlcharent/0.3/iso-pub.ent">
%isogrk1;
%isopub;
]>

<chapter id="input_options" xmlns='http://docbook.org/ns/docbook' xml:lang="en" xmlns:et="http://super-resolution.de/elemtable">
   <title>Input options</title>
<glossary>
   <title/>
   <glossdiv>
      <title>Input fields</title>
      <et:elem desc="Number of input channels" topic="join.ChannelCount">
            This field is used to declare how many <xref 
                  linkend="input_channels">input channels</xref> should be
            made available. New input channel configuration tabs are added
            or removed as needed when this field is changed. It might take
            rapidSTORM a moment to do this, please have patience.
      </et:elem>
      <et:elem desc="Input type" topic="InputType">
            This field is used to select the general type of input that should
            be used. If input should be read from some kind of file, select
            <guilabel>File input</guilabel> here and the specific type of file in the 
            <xref linkend="FileType"/> field.
      </et:elem>
      <et:elem desc="Input file" topic="InputFile">
            The location of the input data is selected in this field. You can put
            any existing file here. It's interpretation (file type) and file-specific
            input options are set in the <xref linkend="FileType"/> field.
            This field is only available when you have selected <guilabel>File</guilabel> as <xref linkend="InputType">input type</xref>.
      </et:elem>
      <glossentry id="FileType">
         <glossterm>File type</glossterm>
         <glossdef><para>
               This field is used to select the type of data that are present in the
               input file. Usually, this is auto-detected, and when the 
               <xref linkend="InputFile">input file</xref> location is changed, this
               field is updated with the auto-determined file type for the selected
               input file.
         </para></glossdef>
      </glossentry>

      <glossentry id="FirstImage">
         <glossterm>First image to load</glossterm>
         <glossdef>
            <para>This option allows skipping of the initial frames of the input. The
               frames are indexed sequentially from 0, and the value given here is 
               inclusive. Consequently, the default value of 0 in this field causes
               all images to be loaded. A value of 5 would skip the first 5 images.</para>
            <glossseealso otherterm="LastImage"/>
         </glossdef>
      </glossentry>

      <glossentry id="LastImage">
         <glossterm>Last image to load</glossterm>
         <glossdef>
            <para>If this option is enabled, it allows limiting the length of the computation.
               All images past the given frame number, counted from 0 and excluding the given
               number, are skipped. For example, a value of 3 would case the four images with
               indices 0, 1, 2 and 3 to be computed.</para>
            <glossseealso otherterm="FirstImage"/>
         </glossdef>
      </glossentry>

      <glossentry id="MirrorY">
         <glossterm>Mirror input data along Y axis</glossterm>
         <glossdef>
            <para>If this option is enabled, each input image is flipped such that the topmost row
               in each image becomes the bottommost row.</para>
         </glossdef>
      </glossentry>

      <glossentry id="DualView">
         <glossterm>Dual view</glossterm>
         <glossdef>
            <para>This option allows splitting an image that contains two logical layers. For example,
               if the reflected beam of a beam splitter is projected onto the left side of the
               camera and the transmitted on the right side, this output allows to separate them into
               two different layers that can be used for 3D estimation.</para>
            <para>The dual view output always splits images exactly in the middle. Use plane
               alignment to configure other shifts.</para>
            <glossseealso otherterm="PlaneAlignment"/>
         </glossdef>
      </glossentry>

      <glossentry id="OnlyPlane">
         <glossterm>Process only one plane</glossterm>
         <glossdef>
            <para>This option allows selecting a single layer from a multi-layer acquisition.
               The default option, <guilabel>All planes</guilabel>, has no effect. All other
               options select a single layer from the available input layers, which will be
               the only layer that is processed.</para>
         </glossdef>
      </glossentry>

      <glossentry id="EstimateBackgroundVariance">
         <glossterm>Estimate background variance</glossterm>
         <glossdef>
            <para>If this option is checked, the background variance (i.e. the variability of
               the background noise) is estimated with the variance of the values 
               below the most common intensity value. If this option is checked, the rapidSTORM
               engine can automatically guess the amplitude threshold.</para>
         </glossdef>
      </glossentry>

      <glossentry id="CameraResponse">
         <glossterm>Camera response to photon</glossterm>
         <glossdef>
            <para>This option allows to give the mean intensity each photon generates on the camera.
               In other terms, a value of 5 in this field assumes that the value of a pixel goes up
               by 5 (on average) every time the pixel is hit by a photon.</para>
         </glossdef>
      </glossentry>

      <glossentry id="DarkCurrent">
         <glossterm>Dark intensity</glossterm>
         <glossdef>
            <para>This option allows to give the dark intensity of the camera, i.e. the mean pixel value
               for a pixel that is hit by no photons during the integration time.</para>
         </glossdef>
      </glossentry>

      <glossentry id="PlaneAlignment">
         <glossterm>Plane alignment</glossterm>
         <glossdef>
            <para>This option allows to describe the mapping from input pixels to sample space coordinates.
               For the mass of single-layer applications, the default choice of <guilabel>No alignment</guilabel>
               is appropriate.</para>
            <para>For multi-layer applications, it is likely that a more complex alignment must be applied here.</para>
            <glossseealso otherterm="NoAlignment"/>
            <glossseealso otherterm="LinearAlignment"/>
            <glossseealso otherterm="SupportPointAlignment"/>
         </glossdef>
      </glossentry>

      <glossentry id="z_calibration_file">
         <glossterm>Z calibration file</glossterm>
         <glossdef>
            <para>The filename of a Z calibration file (<xref linkend="z_calibration_table"/>)
               should be given in this option.</para>
         </glossdef>
      </glossentry>

      <et:elem desc="Join inputs on" topic="join.JoinOn">
            When multiple channels are selected, this field is used to declare how these
            multiple channels are combined into a single image or dataset. The available options are:
            <orderedlist>
            <listitem>Spatial joining in X/Y dimension, which means that images or datasets are pasted
            next to each other. For spatial joining of image data in the X/Y dimension, the not-joined dimension
            (e.g. the Y dimension for X joining) must match.</listitem>
            <listitem>Spatial joining in Z dimension. When processing images, each input channel is 
               considered as a separate layer, with the first channel forming the first layer. For
               localization data, this is identical to X/Y joining in Z dimension.</listitem>
            <listitem>Joining in time. Channels are processed after each other in the sequence of their
               declaration. For images, the dimensions of the images must match.</listitem>
            </orderedlist>
      </et:elem>
      <et:elem desc="Output file basename" topic="OutputBasename">
            This is the location and filename prefix for rapidSTORM output files. More precisely,
            the filenames for rapidSTORM's output files are produced by adding a file-specific suffix
            to the value of this field. 
      </et:elem>
      <et:elem desc="Fluorophore types" topic="FluorophoreTypeCount">
            The number of <xref linkend="fluorophore"/>s present in the input should be given here.
            When multiple fluorophores are selected, additional input fields will appear and allow
            configuration of the spectral parameters.
      </et:elem>
      <et:elem desc="Size of one input pixel" topic="PixelSize">
         This field gives the size of the sample part that is imaged in a single camera pixel.
         Typically, this value should be in the order of 100 nm. See <xref linkend="Thompson2002"/>
         for a discussion about ideal values.
      </et:elem>
      <et:elem desc="PSF FWHM" topic="PSF.FWHM">
            The full width at half maximum of the optical point spread function. More
            precisely, the typical width of an emitter should be entered here, including fluorophore
            size and camera pixelation effects. rapidSTORM will fit spots in the images with a Gaussian 
            with the same FWHM as given here. If the PSF is unknown, it can be determined semi-automatically
            by using the <xref linkend="estimate_psf_form"/> output.
      </et:elem>

      <glossentry id="3DType">
         <glossterm>3D PSF model</glossterm>
         <glossdef>
            <para>
               The 3D PSF model denotes the functional form of the PSF's change with respect to the 
               emitter's Z position. The choices are:
            </para>
            <variablelist>
               <varlistentry><term><guilabel>No 3D</guilabel></term>
                  <listitem><para>The PSF width is constant, and no Z coordinate exists or is fitted.</para></listitem></varlistentry>
               <varlistentry><term><guilabel>Polynomial 3D</guilabel></term>
                  <listitem><para>The polynomial 3D model (see <xref linkend="polynomial_3d"/>) determines
                     PSF widths from the Z coordinate.</para></listitem></varlistentry>
               <varlistentry id="Interpolated3D"><term><guilabel>Interpolated 3D</guilabel></term>
                  <listitem><para>The piecewise cubic 3D model (see <xref linkend="interpolated_3d"/>) determines
                     PSF widths from the Z coordinate.</para></listitem></varlistentry>
            </variablelist>
         </glossdef>
      </glossentry>

   </glossdiv>

   <glossdiv>
      <title>Choices for <xref linkend="FileType"/></title>

      <et:elem desc="Localizations file" topic="localizations_file_input">
      This input driver can be used to read the files written
         by the <xref linkend="localizations_file_output"/> output module.
         The file format is documented with the output module.
      </et:elem>

      <glossentry>
         <glossterm>Andor SIF file</glossterm>
         <glossdef>
            <para>This input driver can read the Andor SIF file format produced
               by Andor Technology software such as SOLIS.</para>
            <para>SIF files are stored in an uncompressed binary format with a
               simple text header. Because reading SIF files cannot be implemented
               in a forward-compatible way (reading new SIF files with old software),
               this driver might be unable to open the file; in this case, an error
               message is shown indicating the maximum known and the encountered
               version of the Andor SIF structure. Please obtain a newer version
               of this software in this case.</para>
         </glossdef>
      </glossentry>

      <glossentry>
         <glossterm>TIFF file</glossterm>
         <glossdef>
            <para>This input driver reads a multipage TIFF stack. All images in the
               TIFF stack must have the same size, and be greyscale images of up to
               64 bit depth. Both integer and floating point data are allowed, even
               though all data are converted internally to 16 bit unsigned format.</para>
         </glossdef>
      </glossentry>
   </glossdiv>

   <glossdiv>
      <title>Choices for <xref linkend="PlaneAlignment"/></title>
      <glossentry id="NoAlignment"><glossterm>No alignment</glossterm>
         <glossdef><para>The upper left pixel is assumed to be at (0,0) nanometers. The 
            <xref linkend="PixelSize"/> field gives the offset of each pixel to the next. For example, if
            the pixel sizes are 100 nm in X and 110 nm in Y, the pixel at (10,15) is at (1,1.65) &mgr;m.
            </para></glossdef></glossentry>
      <glossentry id="LinearAlignment"><glossterm>Linear alignment</glossterm>
         <glossdef><para>Naive pixel coordinates are computed identically to <xref linkend="NoAlignment"/>
            and then transformed by an affine transformation matrix.</para></glossdef></glossentry>
      <glossentry id="SupportPointAlignment"><glossterm>Support point alignment</glossterm>
         <glossdef><para>Naive pixel coordinates are computed identically to <xref linkend="NoAlignment"/>.
            A nonlinear transformation image is read from a file. The naive coordinates are projected
            into the source image of the nonlinear transformation, and the transformation is applied with linear
            interpolation.</para></glossdef></glossentry>
   </glossdiv>

</glossary>
</chapter>
